# ğŸ›°ï¸ Agente RAG para documentaciÃ³n 3GPP (TSpec-LLM)

Este proyecto implementa un agente de preguntas y respuestas tÃ©cnicas usando RAG (Retrieval-Augmented Generation) sobre las especificaciones tÃ©cnicas del estÃ¡ndar 3GPP, utilizando documentos en formato Markdown extraÃ­dos del dataset [TSpec-LLM](https://huggingface.co/datasets/rasoul-nikbakht/TSpec-LLM).

---

## ğŸš€ Â¿QuÃ© hace este agente?

- âœ… Indexa especificaciones 3GPP Release 15 (u otras) usando FAISS
- âœ… Divide los documentos en fragmentos optimizados para recuperaciÃ³n
- âœ… Genera embeddings con modelos de Hugging Face (`MiniLM`)
- âœ… Recupera contexto relevante y responde usando `GPT-4` u otro LLM
- âœ… Funciona localmente o puede conectarse a OpenAI

---

## ğŸ“¦ Requisitos

- Python 3.10 o superior
- Pip

InstalaciÃ³n de dependencias recomendadas:

```bash
pip install -U \
    langchain \
    langchain-community \
    langchain-core \
    langchain-openai \
    langchain-huggingface \
    sentence-transformers \
    faiss-cpu \
    python-dotenv \
    tqdm
```

---

## ğŸ” ConfiguraciÃ³n del entorno

Crea un archivo `.env` en la raÃ­z del proyecto:

```
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

---

## ğŸ§  Â¿CÃ³mo se ejecuta?

1. Coloca los archivos `.md` del dataset 3GPP en la ruta definida en `TSpec-LLM/3GPP-clean/Rel-15`.
2. Construye el Ã­ndice:

```bash
python build_faiss.py
```

3. Ejecuta el agente:

```bash
python ask_agent.py
```

4. InteractÃºa con el agente por consola:


---

## ğŸ›  Estructura de los scripts

- **build_faiss.py**
  - `cargar_documentos()` â†’ Lee todos los archivos Markdown
  - `dividir_en_chunks()` â†’ Divide el contenido en fragmentos de 1000 caracteres
  - `construir_faiss()` â†’ Genera los embeddings y construye el Ã­ndice FAISS
- **ask_agent.py**
  - `cargar_faiss()` â†’ Carga el Ã­ndice previamente guardado
  - `crear_agente()` â†’ Crea el `RetrievalQA` con el modelo `ChatOpenAI`
  - `hacer_pregunta()` â†’ Interfaz conversacional por consola

---

## âš ï¸ Notas y advertencias

- Al usar modelos de OpenAI, asegÃºrate de no exceder tus lÃ­mites de tokens.
- La carga de embeddings puede tomar varios minutos si hay muchos documentos (~700k fragmentos en Release 15).
- Para permitir deserializaciÃ³n segura de tu Ã­ndice FAISS local, asegÃºrate de agregar `allow_dangerous_deserialization=True` al cargar el Ã­ndice.

---

## âœ¨ Futuras mejoras sugeridas

- Interfaz web con `Gradio` o `Streamlit`
- IntegraciÃ³n con modelos locales (`Ollama`, `LLaMA`, etc.)
- Soporte para mÃºltiples versiones (`Rel-15`, `Rel-16`, `Rel-17`...)
- Exportar preguntas y respuestas a CSV

---

## ğŸ“„ Licencia

Este proyecto usa datos de TSpec-LLM, con licencia [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/). El uso de modelos GPT de OpenAI estÃ¡ sujeto a sus [tÃ©rminos de uso](https://openai.com/policies/usage-policies).
